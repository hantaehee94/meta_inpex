import pandas as pd
import numpy as np
import glob
import os
import re
import csv
import io
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sentence_transformers import SentenceTransformer
from sklearn.manifold import TSNE
from adjustText import adjust_text # í…ìŠ¤íŠ¸ ê²¹ì¹¨ ë°©ì§€

# ------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •
# ------------------------------------------------
EXTERNAL_DATA_PATH = "ieee_dataport_all_categories.csv" 
INTERNAL_DATA_DIR = "inpexdata"  # í´ë”ëª… í™•ì¸

# [Mac í°íŠ¸ ì„¤ì •]
plt.rcParams['font.family'] = 'AppleGothic' 
plt.rcParams['axes.unicode_minus'] = False

# ------------------------------------------------
# 2. ë°ì´í„° íŒŒì‹± í•¨ìˆ˜ (Robust Version)
# ------------------------------------------------
def parse_internal_files(directory):
    if not os.path.exists(directory):
        print(f"âŒ '{directory}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    files = glob.glob(str(Path(directory) / "*.csv"))
    parsed_data = []

    print(f"\nğŸ“‚ ë‚´ë¶€ ë°ì´í„° í´ë”ì—ì„œ {len(files)}ê°œ íŒŒì¼ ë°œê²¬. ë¶„ì„ ì‹œì‘...")

    for f_path in files:
        filename = Path(f_path).name
        try:
            with open(f_path, 'r', encoding='utf-8', errors='ignore') as f:
                full_text = f.read()
        except: continue

        if "Zscaler" in full_text or "<!DOCTYPE HTML>" in full_text: continue

        description = ""
        title_candidate = ""
        
        # [ì „ëµ A] JSON íŒ¨í„´
        pattern_desc = re.compile(r'"{1,2}description"{1,2}\s*:\s*"{1,2}(.*?)"{1,2}\s*[,}]', re.IGNORECASE | re.DOTALL)
        match_desc = pattern_desc.search(full_text)
        if match_desc:
            description = match_desc.group(1).replace('""', '"').replace('\\n', ' ')

        if not description:
            pattern_title = re.compile(r'"{1,2}title"{1,2}\s*:\s*"{1,2}(.*?)"{1,2}\s*[,}]', re.IGNORECASE | re.DOTALL)
            match_title = pattern_title.search(full_text)
            if match_title:
                title_candidate = match_title.group(1).replace('""', '"').replace('\\n', ' ')

        # [ì „ëµ B] CSV ìŠ¤íŠ¸ë¦¼ íŒŒì‹±
        if not description:
            try:
                f_io = io.StringIO(full_text)
                reader = csv.reader(f_io)
                for row in reader:
                    if not row: continue
                    first_col = str(row[0]).strip().lower()
                    
                    if any(k in first_col for k in ["abstract", "description", "summary"]):
                        candidates = [c for c in row[1:] if len(str(c).strip()) > 0]
                        if candidates:
                            clean_desc = ", ".join(candidates)
                            if len(clean_desc) > 10:
                                description = clean_desc
                                break 
                    
                    if not title_candidate and any(k in first_col for k in ["title", "full title", "dataset name"]):
                        candidates = [c for c in row[1:] if len(str(c).strip()) > 0]
                        if candidates:
                            title_candidate = ", ".join(candidates)
            except: pass

        final_text = description
        if not final_text or len(final_text) < 5:
            if title_candidate and len(title_candidate) > 2:
                final_text = title_candidate

        if final_text and len(final_text) > 2:
            final_text = final_text.strip(' "').replace('""', '"')
            parsed_data.append({
                'description': final_text,
                'category': 'Internal Asset',
                'source_type': 'Internal (INPEX)',
                'filename': filename
            })

    return pd.DataFrame(parsed_data)

# ------------------------------------------------
# 3. ë°ì´í„° ë¡œë“œ ë° í†µí•©
# ------------------------------------------------
df_internal = parse_internal_files(INTERNAL_DATA_DIR)
if df_internal.empty:
    print("âŒ ë‚´ë¶€ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    exit()

print("ì™¸ë¶€ ë°ì´í„° ë¡œë“œ ì¤‘...")
try:
    df_external = pd.read_csv(EXTERNAL_DATA_PATH)
    col = 'description' if 'description' in df_external.columns else df_external.columns[1]
    df_external = df_external[[col, 'category']].rename(columns={col: 'description'})
    df_external['source_type'] = 'External (Academia)'
    df_external['filename'] = 'IEEE DataPort'
except:
    print("âŒ ì™¸ë¶€ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

df_combined = pd.concat([df_external, df_internal], ignore_index=True)
df_combined['description'] = df_combined['description'].fillna("").astype(str)
df_combined = df_combined[df_combined['description'].str.strip().str.len() > 2]
df_combined = df_combined.reset_index(drop=True)

# ------------------------------------------------
# 4. ì„ë² ë”© ë° ì €ì¥ (Step 2ë¥¼ ìœ„í•œ ì¤€ë¹„) ğŸŒŸ ì¤‘ìš”
# ------------------------------------------------
print("ğŸš€ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(df_combined['description'].tolist(), show_progress_bar=True)

# ë°ì´í„°ì™€ ì„ë² ë”©ì„ íŒŒì¼ë¡œ ì €ì¥ (í”¼í´ ë° ë„˜íŒŒì´ í¬ë§·)
print("ğŸ’¾ ì¤‘ê°„ ë°ì´í„° ì €ì¥ ì¤‘ (processed_data.pkl, embeddings.npy)...")
df_combined.to_pickle("processed_data.pkl")
np.save("embeddings.npy", embeddings)

# ------------------------------------------------
# 5. ì‹œê°í™” (t-SNE)
# ------------------------------------------------
print("cy ìœ„ì¹˜ ì¢Œí‘œ ê³„ì‚° ì¤‘ (t-SNE)...")
tsne = TSNE(n_components=2, perplexity=50, max_iter=1000, random_state=42, n_jobs=-1)
embedding_2d = tsne.fit_transform(embeddings)

df_combined['x'] = embedding_2d[:, 0]
df_combined['y'] = embedding_2d[:, 1]

print("ğŸ¨ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
plt.figure(figsize=(16, 12))

sns.scatterplot(
    data=df_combined[df_combined['source_type'] == 'External (Academia)'],
    x='x', y='y', color='lightgray', s=30, alpha=0.3, linewidth=0, label='External Knowledge'
)

internal_points = df_combined[df_combined['source_type'] == 'Internal (INPEX)']
sns.scatterplot(
    data=internal_points,
    x='x', y='y', color='red', s=150, marker='X', edgecolor='black', label='INPEX Assets'
)

texts = []
for i, row in internal_points.iterrows():
    texts.append(plt.text(
        x=row['x'], y=row['y'], s=row['filename'], 
        fontsize=9, fontweight='bold', color='black'
    ))

print("ğŸ§© í…ìŠ¤íŠ¸ ìœ„ì¹˜ ìµœì í™” ì¤‘...")
adjust_text(texts, arrowprops=dict(arrowstyle='->', color='black', lw=0.5), 
            force_points=0.2, force_text=0.5, expand_points=(1.2, 1.2))

plt.title('INPEX Data Asset Map', fontsize=18)
plt.legend(loc='upper right')
plt.tight_layout()

plt.savefig('INPEX_Map_Final.png', dpi=300)
print("âœ… [Step 1 ì™„ë£Œ] ê·¸ë˜í”„ ì €ì¥ë¨ & ë¦¬í¬íŠ¸ìš© ë°ì´í„° ì €ì¥ë¨.")
plt.show()


"""
# ------------------------------------------------
# 2. INPEX ë‚´ë¶€ ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ (ìµœì¢… ìˆ˜ì •íŒ)
# ------------------------------------------------

import pandas as pd
import numpy as np
import json
import glob
import os
import re
import csv
import io
from pathlib import Path

# ------------------------------------------------
# 2. INPEX ë‚´ë¶€ ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ (ìµœì¢…_ì™„ê²°íŒ)
# ------------------------------------------------
def parse_internal_files(directory):
    if not os.path.exists(directory):
        print(f"âŒ '{directory}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    files = glob.glob(str(Path(directory) / "*.csv"))
    parsed_data = []

    print(f"\nğŸ“‚ ë‚´ë¶€ ë°ì´í„° í´ë”ì—ì„œ {len(files)}ê°œ íŒŒì¼ ë°œê²¬. ì •ë°€ ë¶„ì„ ì‹œì‘...")

    for f_path in files:
        filename = Path(f_path).name
        
        # 1. íŒŒì¼ ì „ì²´ í…ìŠ¤íŠ¸ë¡œ ì½ê¸°
        try:
            with open(f_path, 'r', encoding='utf-8', errors='ignore') as f:
                full_text = f.read()
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì—ëŸ¬ ({filename}): {e}")
            continue

        # 2. ë³´ì•ˆ ì°¨ë‹¨ íŒŒì¼ ê±´ë„ˆë›°ê¸°
        if "Zscaler" in full_text or "<!DOCTYPE HTML>" in full_text:
            print(f"âš ï¸ [Skip] ë³´ì•ˆ ì°¨ë‹¨ëœ íŒŒì¼: {filename}")
            continue

        description = ""
        
        # [ì „ëµ A] JSON íŒ¨í„´ íŒŒì‹± (ë³µì¡í•˜ê²Œ ê¼¬ì¸ JSON ë°ì´í„°ìš©)
        # ì˜ˆ: Historic_Clean_Energy... (CSV ì•ˆì— JSONì´ ë“¤ì–´ìˆëŠ” ê²½ìš°)
        if not description:
            # ì •ê·œì‹: "description": "..." ë˜ëŠ” ""description"": ""..."" íŒ¨í„´ ì°¾ê¸°
            pattern = re.compile(r'"{1,2}description"{1,2}\s*:\s*"{1,2}(.*?)"{1,2}\s*[,}]', re.IGNORECASE | re.DOTALL)
            match = pattern.search(full_text)
            if match:
                description = match.group(1).replace('""', '"').replace('\\n', ' ')

        # [ì „ëµ B] CSV ìŠ¤íŠ¸ë¦¼ íŒŒì‹± (IEA, Australian í”„ë¡œì íŠ¸ íŒŒì¼ìš©) ğŸŒŸ í•µì‹¬ ìˆ˜ì • ğŸŒŸ
        # Pandas ëŒ€ì‹  csv ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ í•œ ì¤„ì”© ìœ ì—°í•˜ê²Œ ì½ìŒ
        if not description or len(description) < 10:
            try:
                # ì´ë¯¸ ì½ì€ í…ìŠ¤íŠ¸(full_text)ë¥¼ ë©”ëª¨ë¦¬ íŒŒì¼ì²˜ëŸ¼ ì·¨ê¸‰
                f_io = io.StringIO(full_text)
                reader = csv.reader(f_io) # csv ëª¨ë“ˆì€ ì¹¸ ìˆ˜ê°€ ë‹¬ë¼ë„ ì—ëŸ¬ ì•ˆ ë‚¨!
                
                for row in reader:
                    if not row: continue # ë¹ˆ ì¤„ íŒ¨ìŠ¤
                    
                    # ì²« ë²ˆì§¸ ì¹¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸° (Abstract, Description ë“±)
                    first_col = str(row[0]).strip()
                    if any(k in first_col.lower() for k in ["abstract", "description", "summary"]):
                        
                        # í‚¤ì›Œë“œ ì´í›„ì˜ ëª¨ë“  ì¹¸ì„ ê²€ì‚¬
                        # (IEA íŒŒì¼ì²˜ëŸ¼ ì‰¼í‘œë¡œ ë¬¸ì¥ì´ ìª¼ê°œì§„ ê²½ìš° ë‹¤ì‹œ í•©ì³ì¤Œ)
                        candidates = [c for c in row[1:] if len(str(c).strip()) > 0]
                        
                        if candidates:
                            # ìª¼ê°œì§„ ë¬¸ì¥ë“¤ì„ ë‹¤ì‹œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
                            clean_desc = ", ".join(candidates)
                            
                            # ë‚´ìš©ì´ ì¶©ë¶„íˆ ê¸¸ë©´(20ì ì´ìƒ) ì±„íƒ
                            if len(clean_desc) > 20:
                                description = clean_desc
                                break
            except Exception as e:
                # print(f"CSV íŒŒì‹± ì—ëŸ¬: {e}") 
                pass

        # 3. ê²°ê³¼ ì €ì¥
        if description and len(description) > 20:
            # ì§€ì €ë¶„í•œ ê¸°í˜¸ ìµœì¢… ì •ë¦¬
            description = description.strip(' "').replace('""', '"')
            parsed_data.append({
                'description': description,
                'category': 'Internal Asset',
                'source_type': 'Internal (INPEX)',
                'filename': filename
            })
        else:
            print(f"âš ï¸ ì„¤ëª… ì¶”ì¶œ ì‹¤íŒ¨ (ë‚´ìš© ì—†ìŒ): {filename}")

    return pd.DataFrame(parsed_data)

# ------------------------------------------------
# 3. Main Logic process for embedding and clustering
# ------------------------------------------------
# (1) ë°ì´í„° ë¡œë“œ
df_internal = parse_internal_files(INTERNAL_DATA_DIR)
print(f"âœ… ë‚´ë¶€ ë°ì´í„°: {len(df_internal)}ê±´")

try:
    df_external = pd.read_csv(EXTERNAL_DATA_PATH)
    desc_col = 'description' if 'description' in df_external.columns else df_external.columns[1]
    title_col = 'title' if 'title' in df_external.columns else None
    keep_cols = ['category']
    if title_col:
        keep_cols.append(title_col)
    keep_cols.append(desc_col)
    df_external = df_external[keep_cols].rename(columns={desc_col: 'description'})
    if title_col:
        df_external = df_external.rename(columns={title_col: 'title'})
    else:
        df_external['title'] = df_external['description']
    df_external['source_type'] = 'External (Academia)'
    df_external['filename'] = 'IEEE DataPort'
except:
    print("âŒ ì™¸ë¶€ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

if not df_internal.empty:
    df_internal = df_internal.copy()
    # ë‚´ë¶€ ë°ì´í„°ëŠ” ë³„ë„ titleì´ ì—†ìœ¼ë¯€ë¡œ íŒŒì¼ëª…ì„ titleë¡œ ì‚¬ìš©
    df_internal['title'] = df_internal['filename']

df_combined = pd.concat([df_external, df_internal], ignore_index=True, sort=False)

# descriptionì´ ë¹„ì–´ìˆê±°ë‚˜ NaNì´ë©´ titleì„ ëŒ€ì‹  ì‚¬ìš©
# (ì™¸ë¶€/ë‚´ë¶€ ë°ì´í„°ì—ì„œ ê²°ì¸¡ ì„¤ëª…ì„ ìë™ ë³´ì™„)
def pick_description(row):
    desc = row.get('description', '')
    if isinstance(desc, str) and desc.strip():
        return desc.strip()
    title = row.get('title', '')
    if isinstance(title, str) and title.strip():
        return title.strip()
    return ""

df_combined['description'] = df_combined.apply(pick_description, axis=1)
df_combined = df_combined[df_combined['description'].astype(str).str.strip() != ""].reset_index(drop=True)
print(f"ğŸ“Š ì´ ë°ì´í„°: {len(df_combined)}ê°œ")

# (2) ì„ë² ë”©
print("ğŸš€ í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘...")
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(df_combined['description'].tolist(), show_progress_bar=True)

# (3) ì°¨ì› ì¶•ì†Œ (t-SNE ì‚¬ìš©)
print("cy ë°ì´í„° ì§€ë„ ê·¸ë¦¬ëŠ” ì¤‘ (t-SNE)...")
# perplexity: ë³´í†µ ë°ì´í„° ìˆ˜ì˜ 1/100 ì •ë„ í˜¹ì€ 30~50 ì‚¬ìš©
# n_jobs=-1: ê°€ëŠ¥í•œ ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš© (ë§¥ë¶ ì„±ëŠ¥ í™œìš©)
tsne = TSNE(n_components=2, perplexity=50, max_iter=1000, random_state=42, n_jobs=-1)
embedding_2d = tsne.fit_transform(embeddings)

df_combined['x'] = embedding_2d[:, 0]
df_combined['y'] = embedding_2d[:, 1]

# (4) ì‹œê°í™”
print("ğŸ¨ ê·¸ë˜í”„ ì¶œë ¥...")
plt.figure(figsize=(14, 9))
sns.scatterplot(data=df_combined[df_combined['source_type'] == 'External (Academia)'],
                x='x', y='y', color='lightgray', s=20, alpha=0.4, linewidth=0, label='External Knowledge')
sns.scatterplot(data=df_combined[df_combined['source_type'] == 'Internal (INPEX)'],
                x='x', y='y', color='red', s=100, marker='X', edgecolor='white', label='INPEX Assets')

plt.title('INPEX Data Asset Map (t-SNE)', fontsize=18)
plt.legend(fontsize=12)
plt.tight_layout()
plt.show()
plt.savefig('INPEX_Map_tSNE.png', dpi=300)
"""